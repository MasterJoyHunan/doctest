### centos 安装 k8s 集群

#### 配置阿里云 yum 源（如果是阿里云的服务器，无需配置）

```bash
# centos6
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo

# centos7
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

# centos8
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo

# 无法使用 wget 则使用 curl
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo

# 备份原先的 repos
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak/

sed -i -e"s|mirrors.cloud.aliyuncs.com|mirrors.aliyun.com|g " /etc/yum.repos.d/CentOS-*
sed -i -e "s|releasever|releasever-stream|g" /etc/yum.repos.d/CentOS-*

yum clean all
yum makecache
yum update
```

#### 安装docker

```bash
# 安装docker所需的工具
yum install -y yum-utils device-mapper-persistent-data lvm2
# 配置阿里云的docker源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 指定安装这个版本的docker-ce
yum install -y docker-ce
# 启动docker
systemctl enable docker && systemctl start docker

cat > /etc/docker/daemon.json << EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
```

#### 设置系统内核

```bash
# 关闭防火墙
systemctl disable firewalld
systemctl stop firewalld
```

```bash
# 修改/etc/sysconfig/selinux文件设置
sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
```

```bash
# 永久禁用，打开/etc/fstab注释掉swap那一行。
swapoff  -a
sed -i 's/.*swap.*/#&/' /etc/fstab
```

```bash
# 修改 iptables 内核参数
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF

cat <<EOT >> /etc/modules-load.d/k8s.conf
overlay
br_netfilter
nf_nat
xt_REDIRECT
xt_owner
iptable_nat
iptable_mangle
iptable_filter
EOT

modprobe br_netfilter ; modprobe nf_nat ; modprobe xt_REDIRECT ; modprobe xt_owner; modprobe iptable_nat; modprobe iptable_mangle; modprobe iptable_filter

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 配置ipvs功能
yum -y install ipset ipvsadm

cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF
chmod +x /etc/sysconfig/modules/ipvs.modules 
bash /etc/sysconfig/modules/ipvs.modules
lsmod | grep -e ip_vs -e nf_conntrack
```

#### 1.24 及以后的版本需要添加 cri-dockerd

```bash
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.2.6/cri-dockerd-0.2.6-3.el7.x86_64.rpm
rpm -ivh cri-dockerd-0.2.6-3.el7.x86_64.rpm
systemctl enable cri-docker

# 重载沙箱 
vim /usr/lib/systemd/system/cri-docker.service  
ExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.8 --container-runtime-endpoint fd://

systemctl daemon-reload
systemctl restart cri-docker
```

#### 安装 k8s

```bash
# 执行配置k8s阿里云源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
# 安装kubeadm、kubectl、kubelet
yum install -y kubectl-1.21.1 kubeadm-1.21.1 kubelet-1.21.1

# 如果上方 yum install 报错使用
yum install -y --nogpgcheck kubelet-1.21.1 kubeadm-1.21.1 kubectl-1.21.1

vim /etc/sysconfig/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"

# 启动kubelet服务
systemctl enable kubelet && systemctl start kubelet
```
#### master 节点 k8s 初始化

```bash
> kubeadm config print init-defaults > kubeadm-init.yaml

# kubeadm-init.yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.170 #修改为本机IP
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock # 修改 cri
  imagePullPolicy: IfNotPresent
  name: k8s-master # 修改
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers # 修改镜像
kind: ClusterConfiguration
kubernetesVersion: 1.28.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.244.0.0/16 # 修改 pod ip
scheduler: {}

> kubeadm init --config=kubeadm-init.yaml
```

最终输出

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.20.213:6443 --token jbchau.il1q081hpyvobxlb \
	--discovery-token-ca-cert-hash sha256:0b604f7455c38cc839448f344dbc8f4dc8808e3f03439c195a0a6061af259345 
```

#### 可能会遇到 coredns 一直无法创建的问题

```bash
kubectl get pods --all-namespaces
NAMESPACE     NAME                          READY   STATUS              RESTARTS   AGE
kube-system   coredns-f9fd979d6-78lmn       0/1     ContainerCreating   0          8m47s
kube-system   coredns-f9fd979d6-kmvzk       0/1     ContainerCreating   0          8m47s
kube-system   etcd-joy                      1/1     Running             0          8m56s
kube-system   kube-apiserver-joy            1/1     Running             0          8m56s
kube-system   kube-controller-manager-joy   1/1     Running             0          8m56s
kube-system   kube-proxy-9lht6              1/1     Running             0          8m47s
kube-system   kube-scheduler-joy            1/1     Running             0          8m56s
```
解决方案：
```shell
# 使用 flannel 或 calico 网络插件，任选一个
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# 使用 flannel 或 calico 网络插件，任选一个
kubectl apply -f https://projectcalico.docs.tigera.io/archive/v3.25/manifests/calico.yaml

vim /etc/kubernetes/manifests/kube-controller-manager.yaml
# 添加下面3个参数
- --allocate-node-cidrs=true
- --cluster-cidr=10.244.0.0/16

# 调试工具
kubectl run -it --restart=Never --image=infoblox/dnstools dnstools
```

###  其他的扩展

无法将某些服务部署至 master 节点

```bash
# 1.24 及以后
kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
# 1.24 以前 
kubectl taint nodes --all node-role.kubernetes.io/master- 
```

Metrics Server

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

#### 无法拉取镜像文件

修改镜像为 `bitnami/metrics-server:<版本>`

#### 启动的时候报错 

```
cannot validate certificate for 192.168.1.170 because it doesn't contain any IP SANs" node="node"
```

##### 解决方案1

在 deploy 添加启动参数 --kubelet-insecure-tls

##### 解决方案2

```bash
> vim /var/lib/kubelet/config.yaml
在文件最后添加参数 `serverTLSBootstrap: true`
> systemctl restart kubelet
```

```bash
> kubectl get csr
NAME        AGE   SIGNERNAME                      REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-2z952   13m   kubernetes.io/kubelet-serving   system:node:k8s-master1   <none>              Pending
csr-56sx7   15m   kubernetes.io/kubelet-serving   system:node:k8s-node2     <none>              Pending
csr-ckts9   14m   kubernetes.io/kubelet-serving   system:node:k8s-node1     <none>              Pending

将上一步获取的 NAME 写入 approve 之后
> kubectl certificate approve csr-2z952 csr-56sx7 csr-ckts9
```

### 重新安装

```bash
kubeadm reset -f
```

### 自建k8s对 LoadBannse无效 因为 LoadBannse 需要服务商提供

所有设置 hostNetwork: true 是一个解决方案

istio-ingress 无法启动

修改configmap kube-proxy 的 mode="ipvs"

#### 创建拉取 docker 凭证

```
kubectl create secret docker-registry aliyun-docker-token \
  --docker-server=<你的镜像仓库服务器> \
  --docker-username=<你的用户名> \
  --docker-password=<你的密码>
  
kubectl create secret docker-registry aliyun-docker-token --docker-server=registry.cn-shenzhen.aliyuncs.com --docker-username=xxx@163.com --docker-password=x -n=xxx
```

### k8s 加入多个主节点

注意，mater 节点建议设置为奇数 3,5,7 。假设只有 2 个 master 节点，当一台 master 节点挂掉后，将导致整个集群不可用。如果有 3 个 master 节点，当一个 master 节点挂掉后，集群依然能正常运行。究其原因是 ETCD 集群的选举机制导致的，apiserver 依赖 ETCD ，如果 ETCD 挂掉了，那 apiserver 将无法提供服务。ETCD 通过 raft 选举机制，只有得到多数节点的投票才能当选为 leader

#### 设置 vip 和 haproxy

略.......

#### 修改 kube-system 命名空间下的  kubeadm-config 的 configmap  

在 ClusterConfiguration 子项上加上 controlPlaneEndpoint，用于访问 apiserver 的 api，最好使用 keeplived 创建一个 vip ，并通过 haproxy 或 nginx 进行负载均衡

```yaml
controlPlaneEndpoint: 192.168.20.100:16443 # VIP 
```

#### 修改 apiserver 证书

如果不修改 apiserver 证书，其他节点加入的时候会出现 Unable to connect to the server: x509: certificate is valid for xxx, not xxx 错误·

```sh
openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt|grep DNS
```

首先备份证书

```sh
mkdir /etc/kubernetes/pki/bk
mv /etc/kubernetes/pki/apiserver.* /etc/kubernetes/pki/bk
```

生成新的证书: 方式1

```sh
kubeadm init phase certs apiserver --apiserver-advertise-address 192.168.20.100 --apiserver-cert-extra-sans 192.168.20.213  --apiserver-cert-extra-sans 192.168.20.214
```

* -- apiserver-advertise-address VIP 
* -- apiserver-cert-extra-sans 节点 IP，可以设置多个

生成新的证书: 方式2 (推荐)

修改 kube-system 命名空间下的  kubeadm-config 的 configmap 的子项 ClusterConfiguration ，在 apiServer 配置里加上 certSANs

```sh
apiServer:
  certSANs:
  - 192.168.20.100 # vip
  - 192.168.20.213 # 实际的 master 节点
  - 192.168.20.214 
  - 192.168.20.215 
```

导出为 clusterConfiguration.yaml

```sh
kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' > clusterConfiguration.yaml
```

生成新的证书

```sh
kubeadm init phase certs apiserver --config clusterConfiguration.yaml
```

#### 加入节点

在老的主节点上执行

```sh
> kubeadm token create --print-join-command
kubeadm join master0:6443 --token 82w0ib.lgltem7jldq30q8l     --discovery-token-ca-cert-hash sha256:7002b790a43be0421dde1c051f56620d7fee87f1f98316f54eb718288f88d4f8 
```

```sh
> kubeadm init phase upload-certs --upload-certs
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
a2fa55f2f331c6d351236b9de7140d06b83107161f5ac657a4bb4b67e0c695fb
```

在需要加入的节点执行

```sh
kubeadm join master0:6443 --token 82w0ib.lgltem7jldq30q8l \
--discovery-token-ca-cert-hash sha256:7002b790a43be0421dde1c051f56620d7fee87f1f98316f54eb718288f88d4f8 \
--control-plane \
--certificate-key  a2fa55f2f331c6d351236b9de7140d06b83107161f5ac657a4bb4b67e0c695fb 
```

* --certificate-key 是老的主节点上执行 `kubeadm init phase upload-certs --upload-certs` 的参数

### k8s 操作 ETCD 

查看 ETCD 集群成员

```sh
docker exec -it 3c8deb56e91c etcdctl --endpoints https://127.0.0.1:2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key member list -w table
```

查看 ETCD 领导

```sh
docker exec -it 3c8deb56e91c etcdctl --endpoints https://127.0.0.1:2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key endpoint status --cluster -w table
```

删除 ETCD 成员

```sh
docker exec -it 3c8deb56e91c etcdctl --endpoints https://127.0.0.1:2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key member remove 77b8aac988011731
```

### 删除节点

驱逐POD

```sh
kubectl drain master2 --delete-local-data --force --ignore-daemonsets
```

删除节点

```sh
kubectl delete node master2
```

>  如果是删除 master 节点还需要删除 ETCD member ，需要进入ETCD 的 leader 节点，执行 `etcdctl member remove xxx`

